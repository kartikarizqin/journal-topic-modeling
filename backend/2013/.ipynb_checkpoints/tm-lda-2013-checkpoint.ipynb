{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "0     implementasi high availability vpn client jari...\n",
      "1     pembangunan media pembelajaran teknik komputer...\n",
      "2     penggunaan sistem ids intrution detection syst...\n",
      "3     rancang bangun aplikasi monitoring jaringan si...\n",
      "4     perancangan jaringan hotspot mikrotik router os 3\n",
      "5     perancangan program simulasi perintah dasar ja...\n",
      "6     jaringan virtual private network keamanan komu...\n",
      "7     pembuatan jaringan local area network laborato...\n",
      "8     sistem keamanan honeypot ids jaringan nirkabel...\n",
      "9     analisis perbandingan load balancing web serve...\n",
      "10    perbandingan unjuk kerja protokol routing ad h...\n",
      "11    analisis qos jaringan multi protocol label swi...\n",
      "12    teknik eksplorasi bukti digital file sharing p...\n",
      "13    perbandingan metode simple queues queues tree ...\n",
      "14    pemanfaatan local area network program netop s...\n",
      "15    membangun privileges jaringan komputer sma neg...\n",
      "16      perancangan jaringan sma muhammadiyah surakarta\n",
      "17    analisis quality service qos jaringan internet...\n",
      "18    perancangan analisis kinerja sistem pencegahan...\n",
      "19    implementasi analisis performa jaringan diskle...\n",
      "20    analisis quality service qos jaringan lokal se...\n",
      "21    rekayasa manajemen jaringan wan smk alhikmah t...\n",
      "22    keefetifan e learning media pembelajaran studi...\n",
      "23    perancangan cetak biru infrastruktur jaringan ...\n",
      "24    analisis kinerja tcp jaringan mobile ad hoc pr...\n",
      "25    network monitoring analysis by packet sniffing...\n",
      "26    kesiapan berwirausaha siswa smk kompetensi tek...\n",
      "27    analisa kinerja jaringan pusat internet pedesa...\n",
      "28    konfigurasi inter vlan cisco graphics user int...\n",
      "29    pagar elektrik keamanan fasilitas lembaga perm...\n",
      "...                                                 ...\n",
      "7681  vulnerability assesment web server www binadar...\n",
      "7682  aplikasi pengamanan data email teknik steganog...\n",
      "7683  program studi pendidikan teknik informatika fa...\n",
      "7684  analysis design information security managemen...\n",
      "7685  kajian yuridis perlindungan hukum nasabah peng...\n",
      "7686  rancang bangun image captcha completely automa...\n",
      "7687  analisis pemanfaatan infrastruktur jaringan wi...\n",
      "7688  perancangan implementasi aplikasi steganografi...\n",
      "7689                infrastruktur wi fi kalbis institut\n",
      "7690  rancang bangun sistem e voting menerapkan hash...\n",
      "7691  analisis kebutuhan pasar layanan server data s...\n",
      "7692  implementasi tools network mapper lokal area n...\n",
      "7693   perlindungan web login sistem algoritma rijndael\n",
      "7694                 examine data safety exelsa website\n",
      "7695  implementasi pengenkripsian penyembunyian data...\n",
      "7696   jaringan internet wireless tanpa bantuan teknisi\n",
      "7697  pengenalan suara manusia metode linier predict...\n",
      "7698                 teknik mengendalikan pc jarak jauh\n",
      "7699                            analisis mdart manet ns\n",
      "7700                 langkah jitu hajar musnahkan virus\n",
      "7701  persepsi auditor senior auditor yunior bekerja...\n",
      "7702  gampang gratis website web personal organisasi...\n",
      "7703  penyisipan teks metode low bit coding media au...\n",
      "7704                 kuliah kerja lapangan profesi kklp\n",
      "7705  radikalisme aksi pergerakan lingkungan studi k...\n",
      "7706                pertolongan pertama insiden hacking\n",
      "7707  perbandingan segmentasi luas nukleus sel norma...\n",
      "7708  rancang bangun aplikasi e reminder services la...\n",
      "7709  kompetensi ti perpustakaan praktek kerja lapan...\n",
      "7710           extreme facebook marketing giant profits\n",
      "\n",
      "[7711 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "fo = pd.ExcelFile('cleaned2013.xls') \n",
    "# clean-data.xlsx\n",
    "df = pd.read_excel(fo, 'Sheet1')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7711\n",
      "[['implementasi', 'high', 'availability', 'vpn', 'client', 'jaringan', 'komputer', 'fakultas', 'hukum', 'universitas', 'udayana']]\n"
     ]
    }
   ],
   "source": [
    "#drop empty line\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "text = df['text']\n",
    "text_list =  [i.split() for i in text]\n",
    "print(len(text_list))\n",
    "print(text_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Biagram & Trigram Models \n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs, minimum count 5 means only that appear 5 times or more.\n",
    "bigram = Phrases(text_list, min_count=5)\n",
    "trigram = Phrases(bigram[text_list], min_count=5)\n",
    "\n",
    "for idx in range(len(text_list)):\n",
    "    for token in bigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_list[idx].append(token)\n",
    "    for token in trigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_list[idx].append(token)\n",
    "            \n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrasedf = pd.DataFrame(text_list)\n",
    "print(phrasedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.5)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build bag of words (corpus)\n",
    "bow = [dictionary.doc2bow(doc) for doc in text_list]\n",
    "\n",
    "print(len(bow))\n",
    "# print(doc_term_matrix[100])\n",
    "\n",
    "# calculate TF-IDF\n",
    "tfidf = models.TfidfModel(bow)\n",
    "corpus_tfidf = tfidf[bow]\n",
    "# similarity = pd.DataFrame(corpus_tfidf, columns=['Topic']).to_csv(\"similarity.csv\")\n",
    "\n",
    "# print(corpus_tfidf)\n",
    "\n",
    "# feature_names = tfidf.get_feature_names()\n",
    "# corpus_index = [n for n in corpus_tfidf]\n",
    "\n",
    "# showme = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)\n",
    "# print(showme)\n",
    "\n",
    "# for i in corpus_tfidf:\n",
    "#     result = pd.DataFrame()\n",
    "#     result = result.append(pd.Series(corpus_tfidf), ignore_index=True)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "#function to compute coherence values\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    np.random.seed(9)\n",
    "    model_list = []\n",
    "    num_topics = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "# num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=2\n",
    "limit=20\n",
    "step=2\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
    "                                                        texts=text_list, start=start, limit=limit, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show graphs\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=18)\n",
    "pprint(model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=18)\n",
    "\n",
    "for idx, topic in model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dominant topic and its percentage contribution in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=bow, texts=text):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        # check notes 28/9\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(model, corpus=bow, texts=text_list)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['No_Dokumen', 'Topik_Dominan', 'Persentase_Kontribusi', 'Kata_Kunci', 'Teks']\n",
    "df_dominant_topic.head(247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe from list and write to xlsx.\n",
    "pd.DataFrame(df_dominant_topic, columns=['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']).to_excel('topics.xls', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_topics = df_dominant_topic['Topik_Dominan'].value_counts()\n",
    "count_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_topics.plot.bar()\n",
    "\n",
    "plt.xlabel('Topik')\n",
    "plt.ylabel('Banyak Judul')\n",
    "plt.title('Persebaran Banyak Judul Penelitian Berdasarkan Topik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The most representative sentence for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "# Mallet\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim;pyLDAvis.enable_notebook()\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(model, corpus_tfidf, dictionary)\n",
    "print(data)\n",
    "pyLDAvis.save_html(data, 'lda-gensim-2013.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud as wd\n",
    "\n",
    "# # lower max_font_size, change the maximum number of word and lighten the background:\n",
    "# wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "# plt.figure()\n",
    "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "for t in range(model.num_topics):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(wd(max_font_size=50, max_words=20, background_color=\"white\", min_font_size=10).fit_words(dict(model.show_topic(t,200))))\n",
    "    plt.axis(\"off\")\n",
    "#     plt.title(\"Topic #\" + str(t))\n",
    "#     plt.savefig(\"wcld-topic-\"+str(t)+\".png\", facecolor='k')\n",
    "#     plt.tight_layout(pad=0)\n",
    "    plt.savefig(\"wcld-topic-\"+str(t)+\".png\", facecolor='none', bbox_inches='tight')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model2013.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "model.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
